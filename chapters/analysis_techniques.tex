\chapter{Data Analysis Techniques}

\section{Fitting}

The distributions of observables that are measured experimentally always contain, at the very least, a degree of statistical uncertainty.
In Physics, we usually model processes with smooth distributions, which can be parametrised with a set of numbers. 
\textit{Fitting} is a process of extraction of parameters from observed distributions. 
It is one of the key tasks of statistical analyses and, consequentially, particle physics measurements. 
Nearly every particle physics analysis features some kind of fitting for the result extraction. 
It is also present in particle trajectory reconstruction algorithms, calorimeter cluster shape parametrisation, and calibration procedures.
One of the most common examples is the fitting of a particle species invariant mass distribution reconstructed from its decay products.
It was, for example, famously used in the discovery of the Higgs boson \cite{ATLAS:2012yve,CMS:2012qbp}, in the  $\H\ra\Z\Zstar\ra 4\ell$ and $\H\ra\g\g$ decay channels.

Fitting consists of two main steps: point estimation and uncertainty (confidence-interval) estimation. 
In the former, a best estimate for a set of parameters is derived, which describes a given dataset.
The latter sets the confidence interval on each parameter estimate in the set. 

The overview here presents only a brief overview of the most relevant method that was used in the work presented in this thesis. 
For a more in-depth consideration of statistical methods in data science and physics, I refer the readers to Refs. \cite{Behnke:2013pga,Blobel_Lohrmann_1998} and for rigorous proofs of the underlying statistical concepts, Refs. \cite{Bohm:2014vmk,James_2006,Barlow:1990vc}. 
The material presented here only summarises the details in these books and articles.
\subsection{Maximum-likelihood method}\label{sec:mle}

One of the most common and popular methods for fitting is the maximum-likelihood method. It is also used here widely in Chapter \todo[inline]{give chapter name}

An observed dataset can be defined as $x = (x_1, x_2,...,x_N)$, with $x_i$ being the result of $N$ independent measurements following an unknown probability density $f(x)$, and $i\in[0,N]$.
The function $f(x)$ is, usually, not known and the shape is often parametrised as $f(x; a)$, where $a=(a_1,...,a_M)$ is a dimension-$M$ vector of unknown-valued parameters. 
For a fitting procedure, one has to construct an \textit{estimator}, which is a function of the observed data that can provide an estimated numerical value, $\hat{a}$, corresponding to the parameter vector $a$.
% Any estimator should satisfy four main criterion:

% \begin{itemize}
%     \item \textbf{Consistency}: as the number of measurements increases, the precision of the estimator increases, i.e.:
%      \begin{equation}
%         \lim_{N\ra\infty}\hat{a} = a,
%      \end{equation}
%     \item \textbf{Bias}: the expectation value of the estimator should be close to the true value of $a$. 
%     If the difference, $b$, is small, it is known as an unbiased estimator:
%     \begin{equation}
%         E[\hat{a}] = a + b,
%     \end{equation}
%     \item \textbf{Efficiency}: The variance of the estimator, $V[\hat{a}]$, should be small. 
%     More formally speaking it should be higher than the inverse of Fisher information, $I(a)$. This is also known as the minimum-variance bound.
%     \begin{equation}
%         V[\hat{a}] \geq \frac{1}{I(a)} \quad \mathrm{with} \quad I_{jk}(a) = -E\left[\sum^N_{i=1}\pdv{f(x_i;a)}{a_j}{a_k}\right],
%     \end{equation}
%     \item \textbf{Robustness}: against wrong inputs or incorrect assumptions.
% \end{itemize}
%Usually, it is difficult to choose an estimator that can satisfy all of these criterion simultaneously. One common estimator is the maximum-likelihood estimator.

A very common technique for an estimator is the so-called maximum-likelihood estimate, which uses the likelihood function.
The likelihood function is built from one- or multi-dimensional PDFs $f(x;a)$ of the measured values $x$:
\begin{equation}\label{eq:likelihood_equation}
    \mathcal{L}(x;a) = \prod_{i=1}^N f(x_i; a).
\end{equation}
% The PDFs used to construct the likelihood function have to be normalised, which implies that the integral of the likelihood function is independent of parameters $a$:
% \begin{equation}
%     \int f(x;a)dx = 1 \quad \ra \quad \int \mathcal{L}(x;a)dx_1dx_2...dx_N = 1.
% \end{equation}
In this case, the maximum-likelihood estimate of the parameters $a$ correspond to $\hat{a}$ for which $\mathcal{L}(x;a)$ is globally maximised.
Because the product of a large number of components can vary over many orders of magnitude, in real-life applications it is more convenient to work with sums.
Therefore, a log-likelihood function is used in practice:
\begin{equation}
    l(x;a) \equiv \ln\mathcal{L}(x;a) = \sum_{i=1}^N \ln f(x_i;a).
\end{equation}
It is worthwhile to note that a logarithm is a monotonic function, therefore the maximum of a function is the same as the maximum of its logarithm. 
The maximum of the log-likelihood function satisfies the standard requirement for an extremum point:

\begin{equation}\label{eq:likelihood_derivative}
    \pdv{l(x;a)}{a_j}=0 \quad \mathrm{for} \quad j=1,...,M.
\end{equation}
The roots of \Cref{eq:likelihood_derivative} are maximum-likelihood estimates of $\hat{a}$. Generally, it is impossible to find an extremum in a large parameter space using analytical methods.
In practice, numerical procedures and dedicated software packages for optimisation are usually used.
Many optimisers used in modern day computers tend to minimise functions, rather than maximise them, therefore a \textit{negative log-likelihood} function is often used.
Maximum-likelihood method is unbiased and consistent as the number of measurements grows, i.e. $N\ra\infty$.
However, it requires a good prior knowledge of the form of the PDF $f(x;a)$, as otherwise the result might be incorrect.

\subsection{Variance of the maximum-likelihood method}\label{sec:mle_variance}


The likelihood function generally can have any non-Gaussian shape, however it can be shown~\cite{James_2006} that in the asymptotic limit, $N\ra\infty$, 
any given $f(x;a)$ can have its likelihood function approximated with a multivariate Gaussian distribution:

\begin{equation}
    \mathcal{L} \propto \exp{-\frac{1}{2}(a-\hat{a})^T H(a-\hat{a})},
\end{equation}
where $H$ is the Hessian matrix of the log-likelihood function. Assuming a good minimum is found (\Cref{eq:likelihood_equation}), the log-likelihood function can be expanded at $a=\hat{a}$ and approximated as a parabola:
\begin{equation}
    l(x;a_1,a_2,...,a_N) = l(x;\hat{a}_1,\hat{a}_2,...,\hat{a}_N) + \frac{1}{2}\sum_{i,k} \pdv{l}{a_i}{a_k} (a_i-\hat{a}_i)(a_k-\hat{a}_k)+...
\end{equation}
In this case, the covariance matrix, $V(\hat{a})$, of the estimated parameter vector is approximated as the inverted Hessian matrix, taken at the maximum-likelihood estimate $\hat{a}$:
\begin{equation}
    V(\hat{a}) = \left[-\pdv[2]{l(x;a)}{a}\middle|^{}_{\ds a=\hat{a}}\right]^{-1} = H^{-1}.
\end{equation}
This gives a symmetric uncertainty for each estimated parameter $a_j$ as:
\begin{equation}
    \hat{\sigma}_{a_j} = \sqrt{\hat{V}_{jj}(\hat{a})}.
\end{equation}
This method is always an approximation of the true covariance matrix, because the likelihood function shape is approximated as a parabola.

A more rigorous approach is to profile the likelihood function in order to calculate a likelihood-based confidence interval.
A profile likelihood ratio for a single parameter, $a_k$, is defined as:
\begin{equation}
    \lambda_{\mu} = \frac{l(x;a_k,\hat{\hat{a}})}{l(x;\hat{a_k},\hat{a})},
\end{equation} 
where the numerator is the log-likelihood for a set of parameters $\hat{\hat{a}}$ estimated for some given value of $a_k$. 
The denominator is the likelihood evaluated at the global extremum of the likelihood. 
The method follows the likelihood ratio function from the minimum to find where it crosses the value corresponding to a desired confidence interval.
In general, this leads to asymmetric uncertainty bars and a different result than uncertainties estimated by the Hessian matrix inversion method.
It is interesting to note, that in the asymptotic limit, both the profiling and Hessian matrix inversion methods give the same results.
However, reevaluating the likelihood for every given value of $a_k$ requires a minimisation of all other parameters, therefore the profiling method can be computationally intensive. 
Therefore, it is often sufficient to apply the Hessian matrix inversion method, as long as additional checks are performed to ensure that the provided uncertainties are accurate.

In particle physics, a commonly used minimisation software is \texttt{Minuit} \cite{James:1975dr,James:2296388}.
It implements the Hessian matrix inversion as the \texttt{HESSE} method and the likelihood-based uncertainty estimation method as \texttt{MINOS}.
\subsection{Extended maximum-likelihood}

In particle physics, it is common to not only parametrise a shape of a distribution of an observable, but also measure the absolute rate (normalisation) of the distribution.
The standard setup for maximum-likelihood method does not allow to determine the absolute normalisation. 
An additional term to the likelihood is to be introduced.
In Nature, if a measurement is performed repeatedly, its rate will fluctuate according to Poissonian statisics.
Hence, the \cref{eq:likelihood_equation}, needs to be have a Poissonian term included:
\begin{equation}\label{eq:extended_likelihood}
    \mathcal{L}(x;a) = \frac{\nu^N}{N!}\exp(-\nu)\prod_{i=1}^N f(x;a),
\end{equation}
where $N$ is the observed number of events and $\nu$ is the expected, or `true', normalisation. 
Such a modified likelihoon is called \textit{extended} likelihood.
Taking the logarithm of \Cref{eq:extended_likelihood} gives:
\begin{equation}\label{eq:extended_log_likelihood}
    l(x;a) = -\nu + N\ln{\nu} + \sum_{i=1}^N \ln (f(x;a)) + C,
\end{equation}
where $C$ is independent of $a$ and $\nu$. 
The extended log-likelihood fitting otherwise follows the same procedure as \Cref{sec:mle,sec:mle_variance}.
As such, when optimising \Cref{eq:extended_log_likelihood} for $a$ and $\nu$, the constant parameter $C$ can be ignored.

\subsection{Unbinned maximum-likelihood fitting}

There are two ways that data can be arranged for a fit: 

\begin{itemize}
    \item Each event enters the likelihood function (\Cref{eq:likelihood_derivative} or \Cref{eq:extended_likelihood}) independently,
    \item Events are first grouped in intervals of the observable $x$ and the count of measurements falling into each interval are provided as inputs to the likelihood function.
\end{itemize}
The intervals are often referred to as \textit{bins}. 
Consequentially, the techniques are referred to as \textit{unbinned} and \textit{binned} fits, respectively.
In this thesis, unbinned maximum-likelihood fits have been used. 
They are computationally more intensive, but are always more statistically-optimal than binned fits. 

Throughout this thesis, fitting is implemented using the \texttt{zfit} framework \cite{ESCHLE2020100508}. 
It provides a Python-based library that is developed to fulfill particle physics fitting requirements.
The \texttt{zfit} framework implements minimisation using the \texttt{iminuit} minimiser \cite{iminuit} which is a Python-friendly implementation of \texttt{Minuit}.

\section{Classification}

When collecting data in measurements, many independent observations (`events') are usually performed in order to get a statistically significant sample.
Resulting datasets generally contain a component of interest, often referred to as \textit{signal}, and many components which might show similar behaviour as the signal in certain distributions. 
The latter is commonly referred to as \textit{background}. Disentangling the signal and background contributions in a given dataset is an extremely important task in Big-Data fields, where there may be hundreds or thousands of sub-components contributing to background that may be misclassified as signal.

In particle physics, the signal component usually refers to a single or a group of decay channels, whose properties are being measured. 
The most straightforward approach is to separate signal and background events by imposing requirements on observables, that are typical or expected for signal.
A requirement on some observable is often referred to as a \textit{selection} or a \textit{cut}.
The downside to this is that there can be non-linear underlying correlations between different selections, which might make them less efficient at background process separation than they could be in conjunction with other observables. 
Therefore, a multidimensional observable space is often desired.

This section introduces the multivariate analyses concepts and boosted decision trees, which is a machine learning technique that can perform classification tasks. 
The material presented here is only a summary detailing techniques used in the analyses presented in the thesis.
For detailed overview of multivariate-classification techniques I refer the reader to to Ref.\cite{Behnke:2013pga}.

\subsection{Multivariate classification}